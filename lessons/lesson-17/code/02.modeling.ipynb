{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, division\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plotly\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import cufflinks as cf\n",
    "\n",
    "init_notebook_mode()\n",
    "cf.go_offline()\n",
    "\n",
    "# Jupyter Notebook Options\n",
    "import warnings\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display as prnt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def table(df,replace_match=\"\",replace_str=\"\"):\n",
    "    return IPython.display.display(HTML(df.to_html().replace('<table border=\"1\" class=\"dataframe\">','<table class=\"table table-striped table-hover\">').replace(replace_match,replace_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'\n",
    "TRAIN_FILE = DATA_DIR + 'train.csv'\n",
    "VALIDATION_FILE = DATA_DIR + 'test.csv'\n",
    "df = pd.read_csv(TRAIN_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv(TRAIN_FILE).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv(VALIDATION_FILE).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.ix[:,'casual':].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "    # Loads the training data, but splits the y from the X\n",
    "    dfx = pd.read_csv(TRAIN_FILE)\n",
    "    return dfx.iloc[:, 0:9], dfx.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = get_train_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Ïµ$ is the RMSLE value (score) $n$ is the total number of observations in the (public/private) data set, $p_i$ is your prediction, and $a_i$ is the actual response for $i$. $log(x)$ is the natural logarithm of $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\epsilon = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\log(p_i + 1) - \\log(a_i+1))^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSLE penalizes an under-predicted estimate greater than an over-predicted estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# First, we should set up some sort of testing framework, so that we can benchmark our progress as we go\n",
    "# The evaluation metric is Root mean squared logarithmic error.\n",
    "def rmsele(actual, pred):\n",
    "    \"\"\"\n",
    "    Given a column of predictions and a column of actuals, calculate the RMSELE\n",
    "    \"\"\"\n",
    "    squared_errors = (np.log(pred + 1) - np.log(actual + 1)) ** 2\n",
    "    mean_squared = np.sum(squared_errors) / len(squared_errors)\n",
    "    return np.sqrt(mean_squared)\n",
    "\n",
    "# This helper function will make a callable that we can use in cross_val_score\n",
    "rmsele_scorer = make_scorer(rmsele, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_value = df['count'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# [expected_value] * len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yhat = np.array([expected_value] * len(df))\n",
    "\n",
    "rmsele(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "# Lets just train a basic model so that we can test if our scoring and\n",
    "# cross validation framework works well. We'll use a Ridge regression,\n",
    "# which is a form of linear regression\n",
    "X, y = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Subset the X to just use temp, atemp, and workingday\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "\n",
    "ridge_estimator = Ridge(normalize=True)\n",
    "\n",
    "scores = cross_val_score(ridge_estimator, Xhat, y, scoring=rmsele_scorer, cv=5)\n",
    "\n",
    "abs(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmsele(y, yhat) / abs(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scores\n",
    "scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "\n",
    "ridge_estimator = Ridge()\n",
    "\n",
    "cross_val_score(ridge_estimator, Xhat, y, scoring=rmsele_scorer, cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is that the case? Higher/Lower for which model?\n",
    "\n",
    "* Values\n",
    "* Coefficients\n",
    "* Penalisation\n",
    "* Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plotly.tools import FigureFactory as FF\n",
    "\n",
    "iplot(FF.create_scatterplotmatrix(df[['temp', 'atemp', 'humidity','count']].sample(2000),width=920,height=700));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill in some of the parameters on cross_val_score\n",
    "def perform_cv(estimator, X, y):\n",
    "    return cross_val_score(estimator, X, y, scoring=rmsele_scorer, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ridge_estimator.fit(Xhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ridge??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FlooredRidge(Ridge):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(FlooredRidge, self).__init__(*args,**kwargs)\n",
    "    \n",
    "    def predict(self, X):        \n",
    "        pred = super(FlooredRidge, self).predict(X) \n",
    "        pred[pred < 0] = 0\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[ridge_estimator.predict(Xhat) > 0].temp.iplot(kind='hist',bins=25,dimensions=(900,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X[ridge_estimator.predict(Xhat) < 0].temp.iplot(kind='hist',bins=25,dimensions=(900,500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# len(range(1,26,2)) * 10 * 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.logspace(0, 3.5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Try a simple grid search with the estimator\n",
    "parameters = {'alpha': np.logspace(0, 3.5, 10),\n",
    "              'normalize' : [True,False]}\n",
    "grid = GridSearchCV(ridge_estimator, parameters, scoring=rmsele_scorer, cv=5)\n",
    "\n",
    "Xhat = X[['temp', 'atemp', 'humidity']\n",
    "# Xhat = X[['temp', 'humidity']\n",
    "\n",
    "grid.fit(Xhat, y)\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_gridscore = pd.concat([pd.DataFrame([x.mean_validation_score for x in grid.grid_scores_]), pd.DataFrame([x.parameters for x in grid.grid_scores_])],axis=1)\n",
    "df_gridscore.set_index(['alpha','normalize']).unstack().iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And for grid_search\n",
    "def perform_grid_search(estimator, parameters, X, y):\n",
    "    grid_search = GridSearchCV(estimator, parameters, scoring=rmsele_scorer, cv=5)\n",
    "    grid_search.fit(X, y)\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "normalize = StandardScaler()\n",
    "normalize.fit(Xhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((df[['temp', 'atemp', 'humidity']] - df[['temp', 'atemp', 'humidity']].mean()) / df[['temp', 'atemp', 'humidity']].std()).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print normalize.scale_\n",
    "print normalize.mean_\n",
    "(Xhat - Xhat.mean()) / Xhat.std()\n",
    "((Xhat - normalize.mean_) / normalize.scale_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Now lets move on to the actual transformation of the inputs\n",
    "# First, not every estimator we'll use will have the \"normalize\" keyword\n",
    "# So let's break it out into a transformer, so that we have better control over it\n",
    "ridge_estimator = Ridge()\n",
    "normalize = StandardScaler()\n",
    "\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "Xhat = normalize.fit_transform(Xhat)\n",
    "\n",
    "scores = perform_cv(ridge_estimator, Xhat, y)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# Now we have the beginnings of a multi-step pipeline\n",
    "# Scikit lets you wrap each of these steps into a Pipeline object,\n",
    "# so you just have to run fit / predict once\n",
    "# instead of manually feeding the data from one transformer to the next\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('normalize', normalize),\n",
    "        ('ridge', ridge_estimator)\n",
    "    ])\n",
    "\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "perform_cv(pipeline, Xhat, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What if you wanted to build your custom transformer?\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "class DivideByTwo(TransformerMixin):\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        return result / 2\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Additionally, you can perform grid search over all of the steps of the pipeline\n",
    "# So you don't have to tune each step manually\n",
    "# The pipeline exposes the underlying steps' parameters like so:\n",
    "# ridge__alpha, and normalize__with_mean\n",
    "\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "\n",
    "parameters = {'ridge__alpha': np.logspace(0, 3, 10)}\n",
    "\n",
    "Xhat = X[['temp', 'atemp', 'humidity']]\n",
    "\n",
    "pipeline = Pipeline([('normalize', normalize), ('ridge', ridge_estimator)])\n",
    "\n",
    "grid = GridSearchCV(pipeline, parameters, scoring=rmsele_scorer, cv=5)\n",
    "grid.fit(Xhat, y)\n",
    "\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_grid_scores(grid, parameter='ridge__alpha', log_scale=True):\n",
    "    idx = map(lambda x: x[0][parameter], grid.grid_scores_)\n",
    "    fig = pd.DataFrame(map(lambda x: x[1], grid.grid_scores_),index=idx).iplot(dimensions=(900,500), asFigure=True)\n",
    "    if log_scale:\n",
    "        fig['layout']['xaxis1'].update({'type':'log'})\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_grid_scores(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.season.sample(2000).iplot(kind='hist', dimensions=(900,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sample(2000).iplot(x='count',y='season',mode='markers',dimensions=(900,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelBinarizer\n",
    "\n",
    "# Lets move on to including more features in our model\n",
    "# We probably want to use a factor like Season in our model, but it's\n",
    "# a categorical feature, and we'll need to convert it to a series of booleans\n",
    "\n",
    "one_hot = OneHotEncoder()\n",
    "# season = one_hot.fit_transform(X['season'].reshape(X.shape[0], 1)).toarray()\n",
    "season = one_hot.fit_transform(X[['season']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_hot.fit_transform(X[['season']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "one_hot.fit_transform(X[['season']]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We then have to join this with the other variables\n",
    "ridge_estimator = Ridge()\n",
    "\n",
    "normalize = StandardScaler()\n",
    "pipeline = Pipeline([('normalize', normalize), ('ridge', ridge_estimator)])\n",
    "\n",
    "season = one_hot.fit_transform(X[['season']]).toarray()\n",
    "Xhat = np.hstack([X[['temp', 'atemp', 'humidity']], season])\n",
    "\n",
    "perform_cv(pipeline, Xhat, y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# There's a faster way of doing this with the argument 'categorical_features', but this is the basic\n",
    "# way through which you would extend the functionality if SciKit Learn\n",
    "\n",
    "class ToArray(BaseEstimator, TransformerMixin):\n",
    "    # We need this because OneHotEncoder returns a sparse array, and normalize requires a non-sparse array\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.toarray()\n",
    "        \n",
    "Xhat = X[['season', 'weather', 'temp', 'atemp', 'humidity']]\n",
    "\n",
    "# I think it needs to be 5 here, because it assumes that '0' is a possible value for an int datatype\n",
    "# Should probably specify the data types in read_csv\n",
    "\n",
    "one_hot = OneHotEncoder(n_values=[5,5], categorical_features=[0, 1])\n",
    "\n",
    "desparse = ToArray()\n",
    "normalize = StandardScaler()\n",
    "ridge_estimator = Ridge()\n",
    "\n",
    "pipeline = Pipeline([('onehot', one_hot),\n",
    "                     ('desparse', desparse),\n",
    "                     ('normalize', normalize),\n",
    "                     ('ridge', ridge_estimator)\n",
    "                    ])\n",
    "perform_cv(pipeline, Xhat, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selective Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StandardScaler??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# OK, so now we've got a pipeline that does one-hot encoding of two categorical variables\n",
    "# and then normalizes the variables\n",
    "# But actually we're not supposed to normalize the the dummy variables.\n",
    "# So we need some way of only normalizing non-dummy variables\n",
    "\n",
    "# Oops, actually the CV splitting converts the Pandas DF to an array, so we can't rely\n",
    "# on the normalize having the proper column names\n",
    "class SelectiveNormalize(StandardScaler):\n",
    "    def __init__(self, cols, copy=True, with_mean=True, with_std=True):\n",
    "        self.cols = cols\n",
    "        super(SelectiveNormalize, self).__init__(copy, with_mean, with_std)\n",
    "    \n",
    "    def fit(self, X, y=None):        \n",
    "        subset = X.iloc[:, self.cols]\n",
    "        return super(SelectiveNormalize, self).fit(subset, y)\n",
    "        \n",
    "    def transform(self, X):\n",
    "        subset = X.iloc[:, self.cols]\n",
    "        normalized = super(SelectiveNormalize, self).transform(subset)\n",
    "        others = [col for col in range(X.shape[1]) if col not in self.cols]\n",
    "        res = np.hstack([X.iloc[:, others],normalized])\n",
    "        return res\n",
    "\n",
    "Xhat = X[['season', 'weather', 'temp', 'atemp', 'humidity']]\n",
    "\n",
    "one_hot = OneHotEncoder(n_values=[5, 5], categorical_features=[0, 1])\n",
    "\n",
    "normalize = SelectiveNormalize([2, 3, 4])\n",
    "desparse = ToArray()\n",
    "ridge_estimator = Ridge()\n",
    "\n",
    "pipeline = Pipeline([('normalize', normalize), ('onehot', one_hot), ('desparse', desparse), ('ridge', ridge_estimator)])\n",
    "perform_cv(pipeline, Xhat, y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Lets try tackling the date column now.The time of day is probably really important\n",
    "# So we need some way of extracting the hour\n",
    "# We'll use a FeatureUnion to do this, to demonstrate the functionality\n",
    "\n",
    "def get_train_data():\n",
    "    # Loads the training data, but splits the y from the X\n",
    "    df = pd.read_csv(TRAIN_FILE, parse_dates=['datetime'])\n",
    "    return df.iloc[:, 0:9], df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SelectColumns(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Passes on a subset of columns from an input ndarray\n",
    "    \"\"\"\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.iloc[:, self.cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExtractHour(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Extracts hour from a datetime series\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        res = np.zeros(X.shape)\n",
    "        for xx in xrange(X.shape[0]):\n",
    "            res[xx] = X.iloc[xx, 0].hour\n",
    "        return res.reshape(res.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CastType(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cast_to):\n",
    "        self.cast_to = cast_to\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.astype(self.cast_to)\n",
    "\n",
    "X, y = get_train_data()\n",
    "\n",
    "# Reminder of the columns:\n",
    "# ['datetime', 'season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed']\n",
    "\n",
    "select_date = SelectColumns([0])\n",
    "select_others = SelectColumns(range(1, 9))\n",
    "\n",
    "cast_float = CastType(np.float64)\n",
    "one_hot = OneHotEncoder(n_values=[5, 5], categorical_features=[0, 3])\n",
    "get_hour = ExtractHour()\n",
    "normalize = SelectiveNormalize(range(2, 8))\n",
    "desparse = ToArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ridge_estimator = Ridge()\n",
    "\n",
    "hour_feature = Pipeline([\n",
    "        ('select_date', select_date),\n",
    "        ('get_hour', get_hour)\n",
    "    ])\n",
    "\n",
    "other_features = Pipeline([\n",
    "        ('select_others', select_others),\n",
    "        ('cast_float', cast_float),\n",
    "        ('onehot', one_hot),\n",
    "        ('desparse', desparse)\n",
    "    ])\n",
    "\n",
    "join_features = FeatureUnion([\n",
    "        ('hour', hour_feature),\n",
    "        ('others', other_features)\n",
    "    ])\n",
    "\n",
    "predict = Pipeline([\n",
    "        ('featurize', join_features),\n",
    "        ('estimator', ridge_estimator)])\n",
    "\n",
    "scores = perform_cv(predict, X, y)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_estimator = RandomForestRegressor(n_estimators=20)\n",
    "\n",
    "predict = Pipeline([\n",
    "        ('featurize', join_features),\n",
    "        ('estimator', rf_estimator)])\n",
    "\n",
    "scores = perform_cv(predict, X, y)\n",
    "abs(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hyperparameter Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for attr in sorted(predict.get_params().keys()):\n",
    "    print attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_estimator = RandomForestRegressor(n_estimators=1)\n",
    "\n",
    "predict = Pipeline([\n",
    "        ('featurize', join_features),\n",
    "        ('estimator', rf_estimator)])\n",
    "\n",
    "params = {\n",
    "    'estimator__max_depth' : [2,3,5,8,10,20,100],\n",
    "    'estimator__min_samples_split' : range(2,10)\n",
    "}\n",
    "\n",
    "grid = perform_grid_search(predict, params, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_grid_scores(grid,'estimator__max_depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_grid_scores(grid,'estimator__min_samples_split')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(df_test, prediction, filename='submission.csv'):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('datetime,count\\n')\n",
    "        submission_strings = df_test.reset_index()['datetime'] + ',' + prediction.astype(str)\n",
    "        for row in submission_strings:\n",
    "            f.write(row + '\\n')\n",
    "\n",
    "# make_submission(df_test, prediction, 'submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
